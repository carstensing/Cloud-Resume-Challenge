
[{"content":"","date":"Jan 22, 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":" Carsten Singleton # Skills \u0026amp; Technologies # Cloud \u0026amp; OS — AWS (S3, Lambda, DynamoDB, API Gateway, IAM, CloudFront, Route 53), Linux IaC \u0026amp; CI/CD — Terraform, GitHub Actions Programming — Python, C++, Bash, JSON Version Control — Git, GitHub Web Dev \u0026amp; Test — Hugo, PyTest Soft Skills — Problem-Solving, Team Collaboration, Clear Communication, Adaptability, Commitment to Quality, Continuous Learning Education # UC San Diego — B.S. Computer Science │ GPA: 3.63 │ Sep 2019 - Mar 2022\nMiraCosta College — A.A. Math and Science │ GPA: 3.96 │ Jan 2017 - Jul 2019\nProjects # Cloud Resume Challenge (Current) # Skills: AWS (S3, Lambda, DynamoDB, API Gateway, IAM, CloudFront, Route 53), Terraform, Linux, Python, PyTest, GitHub, GitHub Actions, Hugo, JavaScript\nDeveloped a portfolio website using Hugo, hosted on AWS S3, with Route 53 for DNS management and HTTPS security, and CloudFront for optimized content delivery.\nCreated Python Lambda function to handle encrypted data transfer from DynamoDB and process REST API requests triggered by the website’s visitor counter via JavaScript.\nAutomated infrastructure provisioning with Terraform and optimized CI/CD pipelines via GitHub Actions, incorporating automated PyTests for validation and seamless deployment.\nPotential Exposure Location Date Updater # Skills: Python, unittest, automation, GitHub\nDeveloped a Python program to parse CSV files, convert valid strings into Date objects, and sort them into appropriate files.\nFollowed test-driven development (TDD) to accelerate development and ensure reliability.\nImplemented logging to detect string formatting errors, making it easier for users to identify and correct issues.\nIntro to Computer Architecture: Custom CPU # Skills: SystemVerilog, Python, CPU architecture, low-level programming language design, GitHub\nDesigned a custom CPU architecture and instruction set in SystemVerilog, supporting a Linear Feedback Shift Register algorithm used for text encryption and decryption.\nDeveloped a low-level programming language for cryptographic programs, representing nine different CPU functions.\nCreated a series of Python scripts that compile programs into custom machine code, featuring self-calculating jumps and comments.\nCertificates # AWS - Certified Cloud Practitioner (Oct 2024 - Oct 2027)\nPCEP - Certified Entry-Level Python Programmer (Feb 2024)\nProfessional Experience # Public Health COVID-19 Compliance Coordinator # University of California San Diego — Feb 2022 - Jun 2023\nDeveloped a Python automation to maintain a data table in UCSD’s COVID-19 Daily Dashboard, improving data consistency and readability, and saving 5+ hours each week.\nCoordinated with four UCSD organizations and local hotels to ensure the safe and efficient transport of COVID-positive students to isolation housing, reducing contagion and maintaining in-person instruction for over 40,000 students.\nAnalyzed contact tracing data to guide staff on Cal/OSHA regulations, minimizing outbreaks and ensuring compliance.\nMath Tutor # MiraCosta College — Aug 2018 - May 2019\nDelivered personalized one-on-one and group tutoring sessions in algebra, trigonometry, and calculus, leading to improved student comprehension, academic performance, and problem-solving skills.\nPromoted a collaborative learning environment by facilitating open discussions and constructive feedback, boosting students\u0026rsquo; confidence and deepening their understanding of complex concepts.\nAdapted teaching methods to effectively support diverse proficiency levels and learning styles, enhancing student comprehension and engagement.\nStudent Ambassador # MiraCosta College — Aug 2017 - Jun 2018\nRepresented MiraCosta College at local schools, engaging with prospective students and informing them about programs and services, boosting student interest and applications.\nSupported prospective students, including underrepresented groups, throughout the application and enrollment process, driving a significant rise in overall enrollment and enhancing diversity.\nLed weekly peer counseling sessions at local high schools, assisting students with the admissions process and providing pre-enrollment advising to ensure a seamless transition to college.\n","date":"Jan 22, 2025","externalUrl":null,"permalink":"/posts/resume/","section":"Posts","summary":"Seeking employment in tech.","title":"Resume","type":"posts"},{"content":"Welcome to my website! I'm really happy you stopped by.\n","date":"Jan 22, 2025","externalUrl":null,"permalink":"/","section":"Welcome to Blowfish!","summary":"Welcome to my website!","title":"Welcome to Blowfish!","type":"page"},{"content":"The Cloud Resume Challenge is a multi-step project designed to help aspiring cloud developers gain real-world experience with cloud technologies by building and deploying a personal resume website.\nThis repo showcases my journey through the challenge and what I learned along the way.\nCheck out my website at carsten-singleton.com.\nIntroduction # Why Learn Cloud # The entry-level job market for roles in software development and IT operations are oversaturated and highly competitive. Traditional education often lacks cloud-specific training, resulting in a shortage of skilled engineers. As more companies depend on cloud services for critical business operations, the demand for jobs outweighs the supply of trained engineers.\nFortunately, cloud engineering is accessible to those without a degree, thanks to numerous certifications and online learning resources. Engineers gain experience in system administration, networking, security, automation, and programming, building a diverse set of highly transferable skills. Additionally, cloud engineering offers competitive salaries, remote work opportunities, and significant potential for career growth.\nThese factors are especially attractive to me, as they support my long-term goal of becoming a DevOps Engineer.\nWhat is the Cloud Resume Challenge # The Cloud Resume Challenge, by Forrest Brazeal, is a project outline that simulates end-to-end cloud development—culminating in a personal resume website. The challenge provides hands-on experience with cloud technologies and serves as a portfolio piece for job seekers in the field. Any cloud service provider can be used to complete this challenge. I chose Amazon Web Services.\nForrest also sells a project guide that details the best ways to go about the challenge and includes additional modifications for even more hand-on practice. I found it to be incredibly helpful.\nGeneral outline:\nCertification: Obtain a cloud certification (AWS Certified Cloud Practitioner).\nFrontend: Create a static website (Hugo) and host it using a cloud provider (S3, Route53 and CloudFront).\nBackend: Implement a visitor counter using a serverless function, database and a REST API (Lambda, DynamoDB and API Gateway).\nInfrastructure as Code (IaC): Automate deployments with Terraform.\nCI/CD: Set up automated testing (pytest and PlayWright) and deployment pipelines (GitHub Actions).\nSteps I Took # 1. AWS Certification # Without any prior cloud experience the project guide was difficult to understand. As I studied for the AWS Certified Cloud Practitioner exam, I learned about vital cloud concepts and the specific services used in the project.\nI spent over 60 hours watching lectures, reading, and answering over 1900 practice problems. Make sure your study material is up to date with the current exam version and outline. I studied old material which set me back some time.\nAfter completing this project, I intend on obtaining the AWS Certified Solutions Architect certification as well.\nFree resources that I used to pass the exam (Oct 2024):\nAndrew Brown\u0026rsquo;s lecture videos Sthithapragna\u0026rsquo;s practice questions 2. Hugo Static Site # I wanted to create a website that I could use for more than my resume. Something simple that worked with Markdown so I could reuse my repository READMEs. My search lead me to a static website framework called Hugo. Hugo builds and serves the site to a localhost, providing a real-time preview of the site. This is super handy for developing locally before moving onto hosting with AWS. Overall, I\u0026rsquo;m happy that I chose to use Hugo over building a basic HTML and CSS static site that I would dread updating.\nFree Resources to learn Hugo:\nHugo documentation Giraffe Academy 3. AWS Organizations # AWS Organizations is a centralized account management service that helps businesses manage multiple AWS accounts efficiently. It provides security, governance, cost optimization, and automation at scale.\nWhat is AWS Organizations? Terminology and concepts for AWS Organizations Okay great, but why did I use it for a single-person project? Besides to get some hands-on experience, here is what I found:\nAccount Protection # Using the AWS root account for development is a bad practice due to its unlimited privileges, making it prone to accidental misconfigurations or deletions that cannot be restricted by IAM policies. It also poses a significant security risk since a compromised root account grants full control over all AWS resources. This can cost thousands of dollars! So to protect yourself from yourself and attackers, develop in a member account, not root.\nIAM Roles and Policies # Member accounts have their permissions set by root. I gave my dev account PowerUserAccess, which grants full access to AWS services and resources, but does not allow management of Users and groups. Sounds like that should be all I need to code some stuff right? Nope! Thanks to IAM roles and policies.\nRoles are like logos that are assigned to users, groups, or services, representing their identity and the level of access they have. Policies are the specific permissions that are attached to a role (or directly to a user or group), defining what actions they can perform.\nImagine a facility with different levels of security. They hire a cleaning company, and anyone with the company logo (role) on their uniform or vehicle is allowed on-site. However, the cleaning crew doesn\u0026rsquo;t have access to the fourth floor of the building. The role is the cleaning company\u0026rsquo;s logo, and the policy is the restriction that they cannot go to the fourth floor.\nThe company\u0026rsquo;s vehicle (which could be used for drop-offs) also has the logo and can enter the premises. However, not just anyone can drive the vehicle; only authorized drivers (users with the correct role) can use it. The driver assumes the role associated with the vehicle and is granted access based on the permissions attached to that role. This is an example of a service role that can be assumed by users to gain temporary access to resources they otherwise wouldn\u0026rsquo;t have.\nAs I moved from one service to another, I found which permissions were lacking. Following the principle of least privilege, I added only the permissions required for each specific task. This incremental approach helped me learn how services use IAM roles and policies, and how they impact development on AWS.\nFree Tier # The hack to creating member accounts is to use email sub-addressing instead of creating a new email. For example, if the root email is myemail@mail.com, the dev account can be created with myemail+dev@mail.com. Even if the root account is no longer eligible for the Free Tier, the member accounts are eligible because they are newly created. After a year, when the dev account loses Free Tier eligibility, simply create another dev account with a different sub-address. Infinite Free Tier!\nBe aware that even with multiple Free Tier accounts within an organization, the benefits don\u0026rsquo;t increase. As long as one or more accounts are Free Tier eligible, the entire organization will share the standard Free Tier benefits.\nAWS Free Tier FAQs SSO Login # To use the AWS Command Line Interface (CLI) locally, account credentials (access keys) must be stored on your computer. However, this poses a significant security risk, since they can accidentally be published to platforms like GitHub. The IAM Identity Center for AWS Organizations provides single sign-on (SSO) access with credentials that expire. These temporary credentials reduce the risk of leaked keys compromising an account. Plus, sso login is very convenient to use, even from the CLI.\nSSO setup guide for personal development To avoid frequent logins, set the session duration to more than an hour.\n4. S3, Route53 and Cloudfront # I purchased a domain name through Route 53 and created an S3 bucket to store my site files built by Hugo. After that, I used CloudFront to enable HTTPS and content delivery. AWS has guides on how to do all of this, which made it very easy.\nLearning Resources:\nConfiguring a static site using a custom domain registered with Route 53 CloudFront and HTTPS Some things weren\u0026rsquo;t so obvious and required some investigation / trial and error. Here is what I learned:\nRoute 53 Name Server Issue # For DNS to work properly, the record for routing traffic to your domain name must use the same name servers that the hosted zone uses. I recreated the hosted zone and record multiple times while following the guide and was auto-generated differing values, which made DNS not work.\nCloudFront Cache Update # CloudFront caches S3 files for the static site, and any updates must be reflected in the cache. The expiration time for cached files be configured, with different lengths of time offering different pros and cons. Read their cache expiration guide for more details.\nSo, even after updating S3, your domain won\u0026rsquo;t serve the updated files until they are cycled out. To avoid waiting for files to expire, they can be manually invalidated or automatically updated if versioned file names are used.\nInvalidations # Transferring new files to the cache incurs costs for both methods, and invalidations have an additional cost after 1,000 submissions. So, it\u0026rsquo;s worth it to be smart about invalidation. Each deleted or modified file must be explicitly listed in the invalidation with its full bucket path. Getting these file paths takes some work but isn\u0026rsquo;t too hard.\nVersioned File Names # AWS recommends versioned file names, but the implementation requires more work since the files names have to be managed. This is a great post about how to use hashed file names to update the cache.\n5. AWS CLI # The AWS CLI tool is awesome because it does things that the online console can\u0026rsquo;t. Certain tasks can be automated with scripts and other are just more efficient. It\u0026rsquo;s also a fantastic learning tool for backwards engineering because the console often does more than one action at a time. I learned this after I redid part 4 only using the CLI.\nCLI install guide CLI command completion CLI command reference SSO for CLI # Some configuration is required to use SSO with the AWS CLI. I prefer editing the ~/.aws/config file directly instead of using their setup wizard. Follow the user guide:\nSSO config for CLI To login:\naws sso login --profile my-profile Automatically loading a profile requires exporting it to your shell startup script:\necho \u0026#34;export AWS_PROFILE=my-profile\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc For safety, it\u0026rsquo;s better to leave AWS_PROFILE unset when using multiple profiles, to ensure actions are performed with the correct profile.\n6. DynamoDB, Lambda, API Gateway and JavaScript # I learned a ton on this chuck of the project. I understood the concept of how a website, a database and an API interacted, but actually building it all really challenged me. Again, since I didn\u0026rsquo;t have any experience with these services, I started with the AWS console.\nI broke everything down into the smallest steps I could and then pieced them together to slowly. I took these steps:\nRead and write to DynamoDB from Lambda TODO https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/programming-with-python.html\n7. pytests # When migrating to local development, I wanted to replicate the integrated test function available in the Lambda online console. Using pytest enabled me to practice test-driven development, automate testing, and accelerate the development process.\nI configured my Python virtual environment to run pytest-watch on startup, which automatically runs tests when the source files are saved. Using pytest-xdist makes it possible to run tests in parallel, speeding up test times. Configuration of pytest.ini is needed to use pytest-xdist automatically when pytest-watch runs the tests.\nTo autorun tests: Add to the venv/bin/activate file: gnome-terminal \u0026ndash; bash -c \u0026ldquo;ptw \u0026ndash;ext=.py,.json\u0026rdquo; If your terminal is different, change \u0026ldquo;gnome-terminal\u0026rdquo;\nTODO\n# pytest.ini [pytest] addopts = -n auto --disable-warnings 8. Terraform # Use the existing AWS infrastructure as a reference when writing Terraform for this project. Create identical resources with Terraform and then replace the original. Leverage AWS CLI commands to gather the required details for Terraform definitions. For example, the get-method CLI command returns the data needed for defining the API Gateway method in Terraform.\nTerraform AWS documentation If you have zero Terraform experience like I did, Rahul Wagh has a fantastic video on how to create a Lambda function is Terraform, which cleared up a lot of confusion.\nFor Lambda, utilize the source_code_hash argument to trigger a rebuild whenever the Python code changes and remember to create an aws_lambda_permission resource for API Gateway when you get there.\nFor API Gateway, remember to take care of the SDK generation. I use the terraform_data resource to run a bash shell script to update the JavaScript files.\n9. Source Control # Git and GitHub\nTechnology Used # AWS CLI\nHugo\npytest\nPlayWright for Python\nTerraform\nGit\nGithub\nVSCode\nUbuntu\nFirefox Developer Tools\n","date":"Oct 17, 2024","externalUrl":null,"permalink":"/posts/cloud_resume_challenge/","section":"Posts","summary":"A work in progress.","title":"Cloud Resume Challenge [Draft]","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]